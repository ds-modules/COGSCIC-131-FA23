{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #c1f2a5\">\n",
    "\n",
    "\n",
    "# PS4\n",
    "\n",
    "# Part 1 \n",
    "\n",
    "In Part 1 of this problem set, we will use the simplest type of a recurrent network, Elman network, to try to learn a simple language. \n",
    "## Instructions\n",
    "\n",
    "\n",
    "\n",
    "Remember to do your problem set in Python 3. Fill in `#YOUR CODE HERE`.\n",
    "\n",
    "Make sure: \n",
    "- that all plots are scaled in such a way that you can see what is going on (while still respecting specific plotting instructions) \n",
    "- that the general patterns are fairly represented.\n",
    "- to label all x and y-axes, and to include a title.\n",
    "    \n",
    "    \n",
    "**Note:** The ideas in this notebook draw heavily from the readings\n",
    "<ol>\n",
    "<li> Elman, J. L. (1990). Finding structure in time. <em>Cognitive Science, 14</em>, 179-211.\n",
    "<li>McClelland, J., & Rumelhart, M. (1986). Past tenses of English verbs. In McClelland, J. and Rumelhart, D. (Eds.) <em>Parallel distributed processing: Explorations in the microstructure of cognition. Vol. 2: Applications</em> (pp. 216-271). Cambridge, MA: MIT Press.\n",
    "</ol>\n",
    "<br>\n",
    "If you are confused about some of the ideas in this notebook or would like further clarification, we recommend having a look there.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Introduction\n",
    "\n",
    "One of the most successful (and controversial) applications of neural\n",
    "        networks has been as models of human language. Specifically, contrary to the rules/symbols approach proposed by Chomsky, neural networks have been used to demonstrate that distributed representations can give rise to language learning. You will test whether a\n",
    "simple neural network is capable of learning the rule underlying a\n",
    "context-free language.\n",
    "\n",
    "The language $a^nb^n$, being the set of all strings containing a\n",
    "sequence of $a$'s followed by a sequence of $b$'s of the same length,\n",
    "is a simple example of a language that can be generated by a\n",
    "**context-free grammar** but not a **finite-state grammar** (because a finite state grammar cannot keep track of the number of times a string has occurred in the sentence). Human languages\n",
    "exhibit similar long-range constraints -- for example, a plural noun\n",
    "at the start of a sentence affects conjugation of a verb at the end,\n",
    "regardless of what intervenes. Some criticisms of applications of\n",
    "neural networks to human languages are based upon their apparent\n",
    "reliance on local sequential structure, which makes them seem much\n",
    "more similar to finite-state grammars than to context-free\n",
    "grammars. In other words, simple neural networks will mostly use the most recent word to predict the next one. An interesting question to explore is thus whether a\n",
    "recurrent neural network can learn to generalize a simple rule\n",
    "characterizing a long-range dependency, such as the rule underlying\n",
    "$a^nb^n$.\n",
    "\n",
    "Recall that an **\"Elman\" network**, as discussed by Elman (1990), is a\n",
    "recurrent network where the activation of the hidden units at the\n",
    "previous timestep are used as input to the hidden units on the current\n",
    "timestep. This type of network architecture allows the\n",
    "network to learn about sequential dependencies in the input data. \n",
    "In this notebook, we will evaluate whether such a network can learn an $a^nb^n$\n",
    "grammar. Here we formalize learning a grammar as being able to correctly\n",
    "predict what the next item in a sequence should be given the\n",
    "rules of the grammar. Therefore, the output node represents the\n",
    "networks's prediction for what the next item in the sequence (the next\n",
    "input) will be -- it outputs a $1$ if it thinks the current input will\n",
    "be followed by an $a$, and outputs a $0$ if it thinks the current\n",
    "input will be followed by a $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Q1. Data\t[5pts, SOLO]\n",
    "We will use the `abdata.npz` dataset for this problem. Make sure that the data file is in the same directory as your notebook while working on the problem set.\n",
    "\n",
    "This dataset has two keys.\n",
    "- The array with key `train_data` contains the sequence we will use to train our network.\n",
    "- The array with key `test_data` contains the sequence we will use to evaluate the\n",
    "network.\n",
    "\n",
    "In both `train_data` and `test_data`, a $1$ represents an $a$ and a $0$ represents a $b$.\n",
    "\n",
    "`train_data` was constructed by concatenating a randomly ordered\n",
    "set of strings of the form $a^nb^n$, with $n$ ranging from 1 to 11.\n",
    "The frequency of sequences for a given value of $n$ in the training set\n",
    "are given by `np.ceil(50/n)`, thus making the frequencies inversely proportional to $n$.\n",
    "The `np.ceil` function returns the smallest integer greater or equal to\n",
    "its input. For example, `np.ceil(3)` is 3, but `np.ceil(3.1)` is\n",
    "4 . \n",
    "\n",
    "`test_data` contains an ordered sequence of strings of the form\n",
    "$a^nb^n$, with $n$ increasing from 1 to 18 over the length of the\n",
    "string.\n",
    "\n",
    "Take a look at the data! You can first print the variables. Next plot the first 100 values of train_data and the first 100 values of test_data in two subplots, as line plots with index numbers on the x axis and data values on the y axis. Upload your plot to gradescope as PS4_Q1.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ab_data = np.load(\"abdata.npz\")\n",
    "ab_data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# look at train_data\n",
    "print('train_data')\n",
    "train_data = ab_data['train_data']\n",
    "print(train_data[:30])\n",
    "print(len(train_data))\n",
    "\n",
    "## look at test_data\n",
    "print('test_data')\n",
    "test_data = ab_data['test_data']\n",
    "print(test_data)\n",
    "\n",
    "## plot the data\n",
    "#YOUR CODE HERE\n",
    "\n",
    "\n",
    "figure.savefig('PS4_Q1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Q2. Input/output [3 pts, HELP]\n",
    "In order to train your network, you will need both training *input* and\n",
    "training *output*. \n",
    "\n",
    "That is, you need a sequence of inputs of the form\n",
    "$a^nb^n$, and a corresponding sequence with the correct output for\n",
    "each item in the input sequence.\n",
    "\n",
    "For this problem we're going to use `train_data[:-1]` as the\n",
    "input training sequence, and `train_data[1:]` as the output\n",
    "training sequence.\n",
    "\n",
    "Explain (1-2 sentences) in gradescope why these are appropriate input and output\n",
    "sequences. If you're confused by what the sequences\n",
    "`train_data[:-1]` and `train_data[1:]` look like,\n",
    "try creating them in a cell and compare them to `train_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Elman network (provided)\n",
    "We have provided you with a function, `train_Elman`, which takes four arguments:\n",
    "- `input` &ndash; the training input sequence\n",
    "- `output` &ndash; the training output sequence\n",
    "- `num_hidden` &ndash; the number of hidden units\n",
    "- `num_iters` &ndash; the number of training iterations: network needs to be trained on many iterations\n",
    "\n",
    "\n",
    "`train_Elman` will:\n",
    "\n",
    "1) create a network with one input node, the specified number of hidden units, and one output node\n",
    "\n",
    "\n",
    "2) train the network on the training data for the specified number of iterations.\n",
    "\n",
    "\n",
    "The network sees the\n",
    "training data one input at a time (in our case, it sees a single $1$\n",
    "or $0$ per time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Elman(inputs, outputs, num_hidden, num_iters):\n",
    "    \"\"\"\n",
    "    Initializes and trains an Elman network. For details see Elman (1990).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : numpy array\n",
    "        A one dimensional sequence of input values to the network\n",
    "\n",
    "    outputs : numpy array\n",
    "        A one-dimensional sequence of desired output values for each of the\n",
    "        items in inputs\n",
    "\n",
    "    num_hidden : int\n",
    "        The number of hidden units to use in the network\n",
    "\n",
    "    num_iters : int\n",
    "        The number of training iterations to run the network for\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    net : dict\n",
    "        Dictionary object containing the trained network weights for each layer. \n",
    "        Key 1 corresponds to the weights from the visibles to the hidden units,\n",
    "        key 2 corresponds to the weights from the hiddens to the output units.\n",
    "\n",
    "    NOTE: Poorly-Python-ported from trainElman.m, which in turn was adapted from\n",
    "    code from http://www.cs.cmu.edu/afs/cs/academic/class/15782-f06/matlab/\n",
    "    recurrent/ which in turn was adapted from Elman (1990) :-)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed=1)\n",
    "\n",
    "    # Parameters\n",
    "    # increment to the derivative of the transfer function (Fahlman's trick)\n",
    "    DerivIncr = 0.2\n",
    "    Momentum  = 0.05\n",
    "    LearnRate = 0.001\n",
    "\n",
    "    num_input  = 1\n",
    "    num_output = 1\n",
    "    num_train  = inputs.shape[0]\n",
    "\n",
    "    if inputs.ndim == 2:\n",
    "        num_input  = inputs.shape[0]\n",
    "        num_output = outputs.shape[0]\n",
    "        num_train  = inputs.shape[1]\n",
    "\n",
    "    if not all([outputs.ndim == inputs.ndim,\n",
    "               inputs.shape[0] == outputs.shape[0]]):\n",
    "        raise ValueError('unequal number of input and output examples')\n",
    "\n",
    "    # create a dictionary to hold the network weights\n",
    "    net = {}\n",
    "    net[1] = np.random.rand(num_hidden, num_input + num_hidden + 1) - 0.5\n",
    "    net[2] = np.random.rand(num_output, num_hidden + 1) - 0.5\n",
    "\n",
    "    # the context layer\n",
    "    # zeros because it is not active when the network starts\n",
    "    Result1 = np.zeros((num_hidden, num_train))\n",
    "\n",
    "    # the row of ones is the bias\n",
    "    Inputs = np.vstack((inputs, np.ones(num_train)))\n",
    "    Desired = outputs\n",
    "\n",
    "    delta_w1 = 0.\n",
    "    delta_w2 = 0.\n",
    "\n",
    "    # Training\n",
    "    for ii in range(num_iters):\n",
    "        # Recurrent state\n",
    "        # includes current inputs, as well as the output of the hidden layer\n",
    "        # from the previous time step\n",
    "        Input1 = np.vstack((Inputs, np.hstack([np.zeros((num_hidden,1)),  Result1[:,:-1]])))\n",
    "\n",
    "        # Forward propagate activations\n",
    "        # input --> hidden\n",
    "        NetIn1 = np.dot(net[1], Input1)\n",
    "        Result1 = np.tanh(NetIn1)\n",
    "\n",
    "        # Hidden --> output\n",
    "        # we again add a row of ones for bias\n",
    "        Input2 = np.vstack((Result1, np.ones(num_train)))\n",
    "        NetIn2 = np.dot(net[2], Input2)\n",
    "        Result2 = np.tanh(NetIn2)\n",
    "\n",
    "        # Backprop errors\n",
    "        # output --> hidden\n",
    "        Result2Error = Result2 - Desired\n",
    "        In2Error = Result2Error * (DerivIncr + np.cosh(NetIn2)**(-2))\n",
    "\n",
    "        # hidden --> input\n",
    "        Result1Error = np.dot(net[2].T, In2Error)\n",
    "        In1Error = Result1Error[:-1, :] * (DerivIncr + np.cosh(NetIn1)**(-2))\n",
    "\n",
    "        # Calculate weight updates\n",
    "        dw2 = np.dot(In2Error, Input2.T)\n",
    "        dw1 = np.dot(In1Error, Input1.T)\n",
    "\n",
    "        delta_w2 = -LearnRate * dw2 + Momentum * delta_w2\n",
    "        delta_w1 = -LearnRate * dw1 + Momentum * delta_w1\n",
    "\n",
    "        net[2] = net[2] + delta_w2\n",
    "        net[1] = net[1] + delta_w1\n",
    "    return net\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 - learn $a^nb^n$ language [5 pts, HELP]\n",
    "Complete the function `anbn_learner` below to train an \"Elman\"\n",
    "network with two hidden units using the provided function `train_Elman` (remember\n",
    "to use the input **train_data[:-1]** and output **train_data[1:]** sequences from Q2).\n",
    "\n",
    "\n",
    "\n",
    "Train the network for *100 iterations*, and return the final output of the network.\n",
    "We provide test cases. If you got the function right, the following cell should print \"Success\". Otherwise, it will give you an error message that will help with debugging.\n",
    "\n",
    "Copy your code into gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c780e1b1346209cf913d01a4babea476",
     "grade": false,
     "grade_id": "anbn_learner",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def anbn_learner(train_data):\n",
    "    \"\"\"\n",
    "    Creates an \"Elman\" neural network with two hidden units and trains it\n",
    "    on the provided data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: numpy array of shape (n,)\n",
    "        the data on which to train the Elman network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    net: dictionary with 2 keys\n",
    "        a dictionary containing the weights of the network. Valid keys are 1 and 2. \n",
    "        key 1 is for the weights between the input and the hidden units, and \n",
    "        key 2 is for the weights between the hidden units and the output units.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8474acbd70334d48c94b2a3f9c2ddd76",
     "grade": true,
     "grade_id": "check_anbn_learner",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that anbn_learner returns the correct output\"\"\"\n",
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal, assert_almost_equal \n",
    "\n",
    "# check that abdata hasn't been modified\n",
    "ab = np.load(\"abdata.npz\")\n",
    "assert_array_equal(test_data, ab['test_data'], \"test_data array has changed\")\n",
    "assert_array_equal(train_data, ab['train_data'], \"train_data array has changed\")\n",
    "\n",
    "# generate test data\n",
    "traindata = np.zeros(20)\n",
    "traindata[10:] = 1.\n",
    "\n",
    "net = anbn_learner(traindata)\n",
    "\n",
    "# check that net has the correct shape and type\n",
    "assert_equal(type(net), dict, \"net should be a dict of network weights\")\n",
    "assert_equal(len(net), 2, \"incorrect number of layers in net\")\n",
    "assert_equal(list(net.keys()), [1,2], \"keys for net should be 1 and 2\")\n",
    "\n",
    "# check the dimensions of the weight matrices\n",
    "assert_equal(net[1].shape, (2,4), \"invalid network weights for the input -> hidden layer\")\n",
    "assert_equal(net[2].shape, (1,3), \"invalid network weights for the hidden -> output layer\")\n",
    "\n",
    "# check the weight matrix sums to the correct value on testdata\n",
    "assert_almost_equal(np.sum(net[1]), -1.9326, places=4, msg=\"weights for input --> hidden layer are incorrect\")\n",
    "assert_almost_equal(np.sum(net[2]), 0.01825, places=4, msg=\"weights for hidden --> output layer are incorrect\")\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Q4 - checking the trained network [5 pts, SOLO]\n",
    "Once the network is trained (*your anbn_learner function should pass the test case in the previous cell)*, you can test it on a new set of sequences\n",
    "and evaluate its predictions to see how well it has learned the target\n",
    " grammar. To generate predictions from the trained network, we use the provided function `predict_Elman`. \n",
    " \n",
    "Use your `anbn_learner` function on `train_data` to train a network, then use your trained network (by passing it as the first input to `predict_Elman` function) to generate output predictions for each of the input elements in `test_data`\n",
    "\n",
    "A) The `predict_Elman` function returns an array of predicted values with the same dimensions as the input. Plot your first 100 predictions (sequence index number on x axis, predicted values on y axis) and upload the figure to gradescope as PS4_Q4.png.\n",
    "\n",
    "B) Look back at your plot of the test data in Q1 - what do you think of the network's predictions? Do your predictions approximate the testing data? In a few sentences, explain why you think they do, or why you think they do not, by referring to the mechanisms of recurrent networks and the nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_Elman(net, inputs):\n",
    "    \"\"\"\n",
    "    Uses the Elman network parameterized by the weights in net to generate\n",
    "    predictions for the elements in inputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    net : dict\n",
    "        Dictionary object containing the trained network weights and\n",
    "        recurrent connections as produced by train_Elman. Key 1 corresponds \n",
    "        to the weights from the visibles to the hidden units, key 2 \n",
    "        corresponds to the weights from the hiddens to the output units.\n",
    "\n",
    "    inputs : numpy array\n",
    "        A one dimensional sequence of input values to the network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    outputs : numpy array\n",
    "        An array containing the predictions generated by the Elman network for the\n",
    "        items in inputs.\n",
    "        \n",
    "    NOTE: Poorly-Python-ported from predictElman.m, which in turn was adapted from\n",
    "    code from http://www.cs.cmu.edu/afs/cs/academic/class/15782-f06/matlab/\n",
    "    recurrent/ which in turn was adapted from Elman (1990) :-)\n",
    "    \"\"\"\n",
    "    num_output = 1\n",
    "    num_hidden = net[1].shape[0]\n",
    "    num_train  = inputs.shape[0]\n",
    "\n",
    "    if inputs.ndim == 2:\n",
    "        num_train = inputs.shape[1]\n",
    "\n",
    "    Inputs = np.vstack((inputs, np.ones([1, num_train])))\n",
    "    Result1 = np.zeros([num_hidden, 1])\n",
    "\n",
    "    outputs = np.zeros(num_train)\n",
    "\n",
    "    for i in range(num_train):\n",
    "        Input1 = np.append(Inputs[:, i], Result1)\n",
    "        NetIn1 = np.dot(net[1], Input1)\n",
    "        Result1 = np.tanh(NetIn1)\n",
    "\n",
    "        Input2 = np.append(Result1, np.ones((1, 1)))\n",
    "        NetIn2 = np.dot(net[2], Input2)\n",
    "        outputs[i] = np.tanh(NetIn2)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "figure.savefig('PS4_Q4.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Q5 - Quantifying the model performance [2pts, HELP]\n",
    "\n",
    "How well does the network do at predicting the next letter? Has it learned the language? Let's look more carefully.\n",
    "\n",
    "To quantify how well the network performs we are going to look at how much the predicted sequence deviates from expectations. The squared error (SE) for a prediction $p_i$ in the prediction vector ${\\bf p}$\n",
    "compared to a target value ${y_i}$ in the target vector ${\\bf y}$ is\n",
    "\n",
    "\\begin{equation}\n",
    "SE_i = (p_i-y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "That is, the squared error is just the squared difference between the\n",
    "predicted and target value.\n",
    "\t\n",
    "Complete the function `squared_error`, which takes in an array of test data and an array of\n",
    "predictions. The function should return an error array **with the same number of elements as the test data**, containing the SE for each of the predictions of the network compared against the corresponding value in `test_data`. \n",
    "\n",
    "Remember that the predictions refer to the _next_ item in the sequence\n",
    "(e.g.  `predictions[0]` should be compared to `test_data[1]`, etc.). You should append an $a$ (coded as a $1$) to the end of your test data to equate the array sizes (describing the start of a new sequence of $a^nb^n$). If the reason for appending $a$ is unclear, think about whether the test data would follow the rules of the grammar if you were to append $b$ instead.\n",
    "\n",
    "We have provided test cases again in the cell below. If your function is right, it should print success. Otherwise, it will give you an error with feedback.\n",
    "\n",
    "Copy your function into gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dc34fc992d440beb6bea797faabbc3e5",
     "grade": false,
     "grade_id": "sq_err",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def squared_error(predictions, test_data):\n",
    "    \"\"\"\n",
    "    Uses equation 1 to compute the SE for each of the predictions made \n",
    "    by the network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions: numpy array of shape (n,)\n",
    "        an array of predictions from the Elman network\n",
    "    \n",
    "    test_data: numpy array of shape (n,) \n",
    "        the array of test data from which predictions were generated\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    se_vector: numpy array of shape (n,)\n",
    "        an array containing the SE for each of items in predictions \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b208daede991b158bc6b80965cd889e0",
     "grade": true,
     "grade_id": "check_sq_err",
     "locked": false,
     "points": 1.25,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that squared_error returns the correct output\"\"\"\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# generate test data\n",
    "pred = np.array([1, 0, 1])\n",
    "test = np.array([0, 1, 0])\n",
    "se = squared_error(pred, test)\n",
    "\n",
    "# check that squared_error returns the correct output for testdata\n",
    "#assert_equal(se.dtype, np.float64, \"squared_error should return an array of floats\")\n",
    "assert_equal(se.shape, (3,), \"squared_error returned an array of the incorrect size on the validate testdata\")\n",
    "assert_array_equal(se, np.zeros(3), \"squared_error should return all zeros on the validate testdata\")\n",
    "\n",
    "# check that squared_error compares the correct elements\n",
    "pred = np.zeros(1)\n",
    "test = np.zeros(1)\n",
    "se = squared_error(pred, test)\n",
    "assert_equal(se, np.ones(1), \"squared_error([0],[0]) should have returned a 1 (did you remember to append an a to testdata?\")\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR OWN TESTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---\n",
    "\n",
    "## Q6 [5 pts, SOLO]\n",
    "Train the network on the `train_data`, then apply it to the `test_data`: try to predict test data using the weights of the trained network. Measure the resulting squared error.\n",
    "\n",
    "Use matplotlib to plot a bar graph of the squared error for each training example. You should have the sequence iteration number on the x axis, and the error values on y axis. Don't forget to provide a title and labels your $x$ and $y$ axes!\n",
    "\n",
    "If you have difficulty interpreting this graph, you may want to\n",
    "examine a few of the values in `test_data`, `predictions`, and your `mse_vector` to see how they are related.\n",
    "Upload your figure PS4_Q6.png to gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "361a23caac2e11f8082d5b4f29f2e4be",
     "grade": false,
     "grade_id": "plot_error",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# create the figure\n",
    "fig, axis = plt.subplots()\n",
    "axis.set_xlim([0.0, 350.0]), axis.set_ylim([0.0,.7])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "axis.bar(np.arange(len(se_vector)),se_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Q7 [5pts, SOLO]\n",
    "\n",
    "To get a better idea of what is going on, let's have a look at the specific values in `test_data` where the prediction error spikes. Use the provided code below and look at its output. \n",
    "\n",
    "At which points in `test_data` do the large errors occur? In 1-2 sentences, explain which parts of the data the network is predicting ok, and which parts it's not predicting well. Answer in gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# prints the 3 values preceding and 2 values following the spot where \n",
    "# the prediction error >= 0.5\n",
    "error_spike_idxs = np.argwhere(se_vector >= 0.5) + 1\n",
    "error_spike_idxs = error_spike_idxs[:-1]\n",
    "\n",
    "for i in error_spike_idxs:\n",
    "    print('3 values preceding MSE spike: {}\\tValue at MSE spike: {}'\n",
    "          '\\t\\t2 values following MSE spike: {}'\\\n",
    "          .format(test_data[i[0]-3:i[0]], test_data[i[0]], test_data[i[0]+1:i[0]+3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "\n",
    "\n",
    "## Q8.1. Conclusions [5pts, SOLO]\n",
    "\n",
    "A) Earlier we said that we can evaluate whether the network has learned\n",
    "the grammar by looking at the predictions it makes. If the network has\n",
    "learned the $a^nb^n$ grammar, in what cases should it make correct\n",
    "predictions? When should it make incorrect predictions?\n",
    "\n",
    "B) Do your predictions about when the network should make correct/incorrect predictions if it has learned the $a^nb^n$ grammar match the the times when the network makes large errors, as identified in Q7? Did the network learn the $a^nb^n$ language?\n",
    "\n",
    "Answer A and B in gradescope in 1-3 sentences each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "\n",
    "## Q8.2. Conclusion [5pts, SOLO]\n",
    "\n",
    "A) At what level of the Chomsky hierarchy is the $a^nb^n$ grammar? How does this compare to the level of most natural languages? Specifically, what level of Chomsky's hierarchy describes most of the natural languages, and are these levels higher relative to that of $a^nb^n$?\n",
    "\n",
    "B) Use this to explore the implications of your results from Q7 and Q8.1 for using the Elman network to model the relationships present in human language. Is the Elman network likely to be sufficient to capture human language satisfactorily?\n",
    "\n",
    "Answer A and B in gradescope in 1-3 sentences each.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #c1f2a5\">\n",
    "\n",
    "\n",
    "# Part 2\n",
    "\n",
    "In this part of the problem set, we will play the number game (Tenenbaum, 2000) that we talked about in the statistical learning lecture. We have a mysterious device that accepts some numbers between 1 and 100 but not others. We will use Bayes' rule to figure out what rule our device might follow and which numbers are likely to be accepted, given some data about the numbers that the device has accepted.\n",
    "    \n",
    "## Instructions\n",
    "\n",
    "\n",
    "\n",
    "Remember to do your problem set in Python 3. Fill in `#YOUR CODE HERE`.\n",
    "\n",
    "Make sure: \n",
    "- that all plots are scaled in such a way that you can see what is going on (while still respecting specific plotting instructions) \n",
    "- that the general patterns are fairly represented.\n",
    "- to label all x- and y-axes and to include a title.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this problem set, consider the following hypotheses for all integers from 1 to 100:\n",
    "- H0: Even numbers from 2 to 100 (2, 4, 6, ..., 100)\n",
    "- H1: Odd numbers from 1 to 99 (1, 3, 5, ..., 99)\n",
    "- H2: Square numbers (1, 4, 9, ..., 100)\n",
    "- H3: Prime numbers (2, 3, 5, ..., 97)\n",
    "- H4: Multiples of 5 (5, 10, 15, ..., 100)\n",
    "- H5: Multiples of 10 (10, 20, 30, ..., 100)\n",
    "- H6: Powers of 2 (2, 4, 8, ..., 64)\n",
    "- H7: All numbers (1, 2, 3, ..., 100)\n",
    "\n",
    "Unless otherwise specified (in Q13), each hypothesis is assumed to be **equally likely to be true** (P(H0) = ... = P(H7) = 1/8) and can be represented as a NumPy array containing acceptable numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. Data\t[5pts, SOLO]\n",
    "\n",
    "To begin, let's create a NumPy array for each of our 8 hypotheses and store the 8 arrays in a list. \n",
    "\n",
    "Then in 8 subplots (1 per hypothesis), plot whether or not the hypothesis has each number from 1 to 100 (x-axis) in it (1 if yes, 0 if not, y-axis). You **MUST** follow the plotting instructions below to get full credit:\n",
    "\n",
    "- Use (16, 10) as your global figure size (`figsize=(16, 10)`)\n",
    "- Arrange 8 subplots in a 4 (rows) $\\times$ 2 (columns) grid\n",
    "- Title each subplot using the corresponding hypothesis label (i.e., \"H1\", \"H2\", etc.)\n",
    "- Label all x- and y-axes in the subplots and title your global figure\n",
    "- Use `axes[i].bar()` instead of `axes[i].plot()`\n",
    "\n",
    "> **Tip**: add `plt.tight_layout()` in the end before saving your plot to avoid overlapping between subplots.\n",
    "\n",
    "Finally, upload your figure in Gradescope as `PS4_Q9.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(n):\n",
    "    \"\"\"Determine if given number if prime\"\"\"\n",
    "    status = True\n",
    "    if n < 2:\n",
    "        status = False\n",
    "    else:\n",
    "        for i in range(2, n):\n",
    "            if n % i == 0:\n",
    "                status = False\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 8 hypotheses\n",
    "H0 = np.arange(2, 101, 2)  # Even numbers from 2 to 100\n",
    "H1 = np.arange(1, 100, 2)  # Odd numbers from 1 to 00\n",
    "H2 = np.arange(1, 11) ** 2  # Square numbers\n",
    "H3 = np.array([i for i in range(101) if is_prime(i)])  # Prime numbers\n",
    "H4 = np.arange(5, 101, 5)  # Multiples of 5\n",
    "H5 = np.arange(10, 101, 10)  # Multiples of 10\n",
    "H6 = 2 ** np.arange(1, 7)  # Powers of 2\n",
    "H7 = np.arange(1, 101)  # All numbers\n",
    "H = [H0, H1, H2, H3, H4, H5, H6, H7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "figure.savefig(\"PS4_Q9.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10. Likelihood [10 pts HELP]\n",
    "\n",
    "In this question, you will be computing the likelihood of data under a given hypothesis. Your data will be represented as an array (e.g., [3, 19, 63]).\n",
    "\n",
    "To do so, write a function called `compute_likelihood` that takes a NumPy array (each number is a **unique** integer between 1 and 100) and a hypothesis (H0-H7, as specified in Q9) as inputs and returns 1) an array containing the likelihood of each number independently (e.g., the likelihoods of 3, 19, and 63 separately) as well as 2) the likelihood of the entire array (e.g., the likelihood of [3, 19, 63]). \n",
    "\n",
    "Please think back on the **size principle** when calculating likelihoods and assume that each number allowed by a particular hypothesis is equally likely. \n",
    "\n",
    "1. In Gradescope, write down what likelihood each of the 8 hypotheses assigns to a number accepted by it and a number that is not accepted.\n",
    "\n",
    "2. In 8 subplots (1 per hypothesis), plot the likelihood of each individual number between 1 and 100 under that hypothesis. You **MUST** follow the plotting instructions below to get full credit:\n",
    "\n",
    "- Use (16, 10) as your global figure size (`figsize=(16, 10)`)\n",
    "- Arrange 8 subplots in a 4 (rows) $\\times$ 2 (columns) grid\n",
    "- Title each subplot using the corresponding hypothesis label (i.e., \"H1\", \"H2\", etc.)\n",
    "- Label all x- and y-axes in the subplots and title your global figure\n",
    "- Use `[0, .2]` as the y-axis limit in each subplot\n",
    "- Use `axes[i].bar()` instead of `axes[i].plot()`\n",
    "\n",
    "> **Tip**: add `plt.tight_layout()` in the end before saving your plot to avoid overlapping between subplots.\n",
    "\n",
    "Finally, upload your figure in Gradescope as `PS4_Q10.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "figure.savefig(\"PS4_Q10.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11 - Bayes' rule [10 pts SOLO]\n",
    "\n",
    "Now let's calculate the posterior probability of each hypothesis in light of data. Below are 8 datasets you'll be working with:\n",
    "\n",
    "- (a) No data \n",
    "- (b) 50\n",
    "- (c) 53\n",
    "- (d) 50, 53 \n",
    "- (e) 16\n",
    "- (f) 10, 20\n",
    "- (g) 2, 4, 8\n",
    "- (h) 2, 4, 8, 10\n",
    "\n",
    "As mentioned in the very beginning, let's assume that each of our 8 hypotheses is equally likely prior to seeing data and that, under each hypothesis, each number allowed by it is also equally likely (the \"size principle likelihood\").\n",
    "\n",
    "Write a function called `compute_posterior` that computes the posterior probability of each hypothesis given a dataset. Note that the 8 probabilities should sum to 1. Please check to make sure that's the case!\n",
    "\n",
    "Your function should take three arguments &mdash; a dataset (a NumPy array), a set of hypotheses (a list), and the prior probabilities of the hypotheses (a NumPy array) &mdash; and return an array containing the posterior probability of each hypothesis.\n",
    "\n",
    "In 8 subplots (1 per dataset), plot the posterior probability of each hypothesis. You **MUST** follow the plotting instructions below to get full credit:\n",
    "\n",
    "- Use (16, 10) as your global figure size (`figsize=(16, 10)`)\n",
    "- Arrange 8 subplots in a 4 (rows) $\\times$ 2 (columns) grid\n",
    "- Title each subplot using the corresponding dataset label (i.e., \"D1\", \"D2\", etc.)\n",
    "- Label all x- and y-axes in the subplots and title your global figure\n",
    "- Use `axes[i].bar()` instead of `axes[i].plot()`\n",
    "\n",
    "> **Tip**: add `plt.tight_layout()` in the end before saving your plot to avoid overlapping between subplots.\n",
    "\n",
    "\n",
    "Finally, upload your figure in Gradescope as `PS3_Q11.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the 8 datasets\n",
    "DataSets = []\n",
    "\n",
    "D0 = np.empty(0)\n",
    "DataSets.append(D0)\n",
    "\n",
    "D1 = np.array([50])\n",
    "DataSets.append(D1)\n",
    "\n",
    "D2 = np.array([53])\n",
    "DataSets.append(D2)\n",
    "\n",
    "D3 = np.array([50, 53])\n",
    "DataSets.append(D3)\n",
    "\n",
    "D4 = np.array([16])\n",
    "DataSets.append(D4)\n",
    "\n",
    "D5 = np.array([10, 20])\n",
    "DataSets.append(D5)\n",
    "\n",
    "D6 = np.array([2, 4, 8])\n",
    "DataSets.append(D6)\n",
    "\n",
    "D7 = np.array([2, 4, 8, 10])\n",
    "DataSets.append(D7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "figure.savefig(\"PS4_Q11.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q12 - Posterior predictive [5 pts, HELP]\n",
    "\n",
    "_\"Yesterday's posterior is today's prior.\"_ Once you've updated your prior probabilities based on data, you can use the new posterior probabilities to make predictions for new data, which we call \"[posterior predictives](https://en.wikipedia.org/wiki/Posterior_predictive_distribution)\". Since we don't know which hypothesis is true, the posterior predictive probability should be marginalized over all hypotheses ($D$: observed data; $\\tilde {d}$: new data; hypothesis: $h \\in H$): \n",
    "\n",
    "$${\\displaystyle p({\\tilde {d}}|\\mathbf {D} )= \\sum_{h\\in H} p({\\tilde {d}}|h ,\\mathbf {D} )\\,p(h |\\mathbf {D})}.$$\n",
    "\n",
    "Write a function called `posterior_predictive` that returns the posterior predictive probability of each number from 1 to 100 marginalized over 8 hypotheses. Your function should take 4 inputs &mdash; numbers from 1 to 100 (a NumPy array), a dataset (a NumPy array), a set of hypotheses (a list), priors over the hypotheses (a NumPy array) &mdash; and return an array containing each number's posterior predictive probability. You'll be working with the same 8 datasets from Q11.\n",
    "\n",
    "In 8 subplots (1 per dataset), plot the posterior predictive probability (marginalizing over all hypotheses) for each number from 1 to 100 &mdash; that is, the predicted probability that each number will be accepted by the true hypothesis, which is unknown (hence the marginalization). You **MUST** follow the plotting instructions below to get full credit:\n",
    "\n",
    "- Use (16, 10) as your global figure size (`figsize=(16, 10)`)\n",
    "- Arrange 8 subplots in a 4 (rows) $\\times$ 2 (columns) grid\n",
    "- Title each subplot using the corresponding dataset label (i.e., \"D1\", \"D2\", etc.)\n",
    "- Label all x- and y-axes in the subplots and title your global figure\n",
    "- Use `[0, .2]` as the y-axis limit in each subplot\n",
    "- Use `axes[i].bar()` instead of `axes[i].plot()`\n",
    "- Your x-axis should be the numbers between 1 and 100 and your y-axis is the probability you computed\n",
    "\n",
    "> **Tip**: add `plt.tight_layout()` in the end before saving your plot to avoid overlapping between subplots.\n",
    "\n",
    "\n",
    "Finally, upload your figure in Gradescope as `PS4_Q12.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "figure.savefig(\"PS4_Q12.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q13.1 - Testing other hypotheses [2 pts, SOLO]\n",
    "\n",
    "In this question, you will re-make the plots from Q12 but now incorporate \"range-based hypotheses\". \n",
    "\n",
    "To do this, let's assume that the 8 hypotheses H0-H7 each have a prior of 1/9 and the remaining 1/9th of the total probability is distributed equally among all intervals in the range 1-100. Here we will define an \"interval\" as something containing two distinct points, such as [50-51] or [3-88] (first number is smaller than the second), but not [31]. To calculate each number's likelihood within each interval, keep on using the size principle prior.\n",
    "\n",
    "> **Tip**: you can use `permutations` from `itertools` to get all permutations of 1-100 of length 2 (but then you only need to keep pairs where the first number is smaller than the second)\n",
    "\n",
    "How many range-based hypotheses are there? (There are a lot!) Enter your answer (numerical value) in Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q13.2 - Testing other hypotheses [8 pts, SOLO]\n",
    "A. Re-plot Q12 (check the instructions above) but with new range-based hypotheses taken into account. Upload your new figure as `PS4_Q13.png` in Gradescope.\n",
    "\n",
    "B. For each subplot, write a sentence to comment on whether including range-based hypotheses makes it better match your own intuitions about how people generalize rules from observed data and explain why you think so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "figure.savefig(\"PS4_Q13.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #c1f2a5\">\n",
    "\n",
    "# Submission\n",
    "\n",
    "    \n",
    "When you're done with your problem set, do the following:\n",
    "- Upload your answers in Gradescope's PS4.\n",
    "- Convert your Jupyter Notebook into a `.py` file by doing so:    \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "<center>    \n",
    "  <img src=\"../img/py_exporting_instructions.png\" width=\"500\"/>\n",
    "</center>\n",
    "\n",
    "<div style=\"background-color: #c1f2a5\">\n",
    "    \n",
    "- Submit the `.py` file you just created in Gradescope's PS4-code.\n",
    "    \n",
    "</div>        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
